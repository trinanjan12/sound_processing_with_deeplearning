{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My notes for reference . check out the original work by https://github.com/musikalkemist/AudioSignalProcessingForML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T11:11:21.909150Z",
     "start_time": "2021-01-17T11:11:21.848675Z"
    }
   },
   "source": [
    "# Table of contents\n",
    "    1. The Basics of Sound \n",
    "        - what is sound and it's wave property\n",
    "        - freq,amplitude,phase of a sine wave\n",
    "        - pitch vs freq relation\n",
    "    2. Intensity, Loudness, Timbre\n",
    "        - power and intensity measurement\n",
    "        - loudness measurement\n",
    "        - what is timbre and it's properties\n",
    "    3. Understanding audio signals\n",
    "        - sampling,nyquist rate,aliasing\n",
    "        - quantization,bit depth\n",
    "    4. Audio Feature\n",
    "        - brief introduction to time domain audio feature\n",
    "        - brief introduction to freq domain audio feature\n",
    "        - spectral leakage, windowing and overlapping frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.The Basics of Sound :\n",
    "\n",
    "Sound is produced by the vibration of an object which in turns vibrates the air molecules to oscillate. The change of pressure in the air molecule creates an wave that is interpreted by our ear.\n",
    "\n",
    "The basic waveform of sound looks like this where we have amplitude in the y axis and time in the x axis.\n",
    "\n",
    "![sound_waveform_l1](./images/sound_waveform_l1.png)\n",
    "\n",
    "\n",
    "### Sine wave different terminology:\n",
    "\n",
    "**amplitude** : The Amplitude is the height from the centre line to the peak (or to the trough). Or we can measure the height from highest to lowest points and divide that by 2.\n",
    "\n",
    "**phase** : The Phase Shift is how far the function is shifted horizontally from the usual position.\n",
    "\n",
    "**frequency/period** : when we say a sine wave has a freq of 22khz that means it has 22k samples(peak to peak is one sample) in one second. \n",
    "period = 1/freq which means how long does one sample takes in second \n",
    "\n",
    "good source for sine wave terminology: https://www.mathsisfun.com/algebra/amplitude-period-frequency-phase-shift.html\n",
    "\n",
    "![sine_wave](./images/sine_wave.gif)\n",
    "\n",
    "![hearing_range](./images/hearing_range.png)\n",
    "\n",
    "\n",
    "**generally higher the amplitude results in louder sound and higher frequency results in higher sound(higher pitch)**.\n",
    "\n",
    "good source for pitch/amplitude:  https://www.phys.uconn.edu/~gibson/Notes/Section2_1/Sec2_1.htm\n",
    "\n",
    "**Pitch vs Freq** : The perception of pitch is logarithmic in nature. For example let's say we are playing C1 note of piano which has a pitch A0, now if we double the freq we will land in C2 and the resultant pitch will be A1, thus as we go one octave above the pitch doubles. So it follows the following log scale \n",
    "\n",
    "![pitch_freq](./images/pitch_freq.png)\n",
    "\n",
    "\n",
    "**TODO**\n",
    "provide explanation for the below parts\n",
    "\n",
    "each octave again could be divided into Cents.\n",
    "http://hyperphysics.phy-astr.gsu.edu/hbase/Music/cents.html#:~:text=Musical%20intervals%20are%20often%20expressed,then%20the%20interval%20is%20cents.\n",
    "\n",
    "Why sound could be represented by sin wave, why not any other wave ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T15:45:13.777519Z",
     "start_time": "2021-01-17T15:45:13.772819Z"
    }
   },
   "source": [
    "# 2. Intensity, Loudness, Timbre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sound power and intensity**  \n",
    "sound power is basically the rate at which energy is transferred, Energy per unit of time emitted by a sound source in all directions. It is measured in **watt(W)**\n",
    "\n",
    "sound power per unit area becomes **sound intensity** and measured in W/m2\n",
    "\n",
    "Threshold of hearing or TOH for human is 10*-2 W/m2\n",
    "Threshold of pain or TOP for human is 10 W/m2\n",
    "\n",
    "intensity level of a sound is basically measured with reference to TOH and it is represented in db(decibels) in log scale\n",
    "\n",
    "for example if we have a sound with intensity I we can measure the db of the sound using the formula below\n",
    "\n",
    "![intensity_formula](./images/intensity_formula.png)\n",
    "\n",
    "**Every 3 dBs, intensity doubles**\n",
    "\n",
    "**Loudness of sound**:<br>\n",
    "how loud is some sound, basically a perception of the pitch that we are hearing. Depending on age this could change. It is dependent on the duration of the sound and the freq of the sound. \n",
    "\n",
    "For example 2 sound may have same intensity but depending on how long they sustain, the loudness will vary.\n",
    "\n",
    "**Loudness is measured in phons**\n",
    "\n",
    "below is graph of freq vs sound intensity(db). We can see from the graph that different lines have different phons. If we consider the 80phon line we can see that for very low freq we need high intensity to generate 80 phons\n",
    "\n",
    "![loudness_conture](./images/loudness_conture.png)\n",
    "\n",
    "**Timbre of sound**:<br>\n",
    "Timbre doesn't have a solid definition. We could say that timbre is color of sound. Let's say we are playing same note in 2 different instrument with same intensity,freq and duration we could still hear them differently.sometime it is described as bright, dark, dull, harsh, warm\n",
    "\n",
    "Timbre is multidimensional. To understand timbre we need to understand the following things\n",
    "\n",
    "**Sound envelope**:<br>\n",
    "In sound and music, an envelope describes how a sound changes over time. we have the Attack-Decay-Sustain-Release Model. Let's say we hit the C4, which will have a initial strike/attack phase then decay, then sustain and then release. \n",
    "![envelop_model](./images/envelop_model.png)\n",
    "\n",
    "If you compare 2 different instrument like piano and violin, we could see that violin has longer attack and smooth attack time/not as sharp as piano.\n",
    "\n",
    "![envelop_model_example](./images/envelop_model_example.png)\n",
    "\n",
    "\n",
    "**Complex sound**:<br>\n",
    "we could think of complex sound as superpositions of different sinusoids. each unique sinusoid is a **partial**. the minimum/lowest partial is **fundamental freq** and a **harmonic partial** is a frequency that is a multiple of the fundamental\n",
    "frequency\n",
    "for example if we have fundamental freq as 440hz the harmonic partials are 2* 440, 3* 440, 3* 440\n",
    "\n",
    "**Frequency modulation/amplitude modulation**:<br>\n",
    "modulating the freq is vibrato. amplitude modulation is tremolo. Both are used for expressive purposes.\n",
    "\n",
    "![envelop_model_example](https://upload.wikimedia.org/wikipedia/commons/a/a4/Amfm3-en-de.gif)\n",
    "\n",
    "All these things above Sound envelope/Complex sound/Frequency modulation/amplitude modulation describes the **timbre** of the music"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Understanding Audio signals\n",
    "\n",
    "Audio signals are continuous **analog** waves, so the question is how do we store it in a digital form? \n",
    "We sample the signal at equi-distance interval and store the values.\n",
    "\n",
    "**Pulse-code modulation (PCM)** is a method used to digitally represent sampled analog signals. It is the standard form of digital audio in computers, compact discs, digital telephony and other digital audio applications. In a PCM stream, the amplitude of the analog signal is sampled regularly at uniform intervals, and each sample is quantized to the nearest value within a range of digital steps.\n",
    "\n",
    "![sampling_audio](https://www.technologyuk.net/telecommunications/telecom-principles/images/pcm01.gif)\n",
    "\n",
    "to locate the sample(basically to find out when the sample n occurs) we can use the formula <br>\n",
    "$$tn = n.T$$\n",
    "\n",
    "**sampling rate**:<br>\n",
    "rate at which we sample the analog audio. obviously the more sample we take the better we can represent the original signal. \n",
    "let's say we have one signal with freq **F** abd we want to sample it. question is how do we choose the sampling rate so that the digital signal will be a good representation of the original sample. we can decide the sampling freq using *Nyquist theorem*\n",
    "\n",
    "**Nyquist theorem**:<br>\n",
    "it says that if we are sampling a audio signal of **f** freq we need to at least sample at a rate of **2f**. which means we need to sample atleast 2 points for each sample. if we don't maintain this the newly form wave will not be able to represent the original wave, and we will get a **aliased** waveform.\n",
    "\n",
    "For exam let's say we have one waveform of 44khz and we start sampling it at 1000 hz. what this means is that all the freq above 500 hz will not get captured, and we will lose information beyond 500 hz of the original freq\n",
    "\n",
    "good resource for sampling/nyquist rate/aliasing https://www.tutorialspoint.com/digital_communication/digital_communication_sampling.htm\n",
    "\n",
    "**Quantization**:<br>\n",
    "similar to sampling rate we apply quantization in the y axis which is amplitude. this tells to store amplitude how many bits are needed. so the resolution of quantization is measured in bit.\n",
    "![quantization](./images/quantization.png)\n",
    "when we see bit depth is written in the CD rom, that means it can represent amplitude of value 2^16=65536\n",
    "\n",
    "**how much memory is needed to store 1 min worth of audio signal in a CDROM which has sampling rate 44100hz and Bit depth = 16 bits.**\n",
    "\n",
    "==> so there are 44100 samples and each can be represented by 16 bits. so total number of bits in 1 sec(since 44100 samples are present in 1 sec) is 16 * 44100 bits . for 60 sec it becomes 16 * 44100 * 60 bits or 5.49 mega byte\n",
    "\n",
    "**when we record sound to digitize we basically use a ADC or analog to digital convertor which applies sampling/quantization/and some low pass filter(to remove freq above nyquist freq) to convert the signal**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Audio Features\n",
    "\n",
    "we extract relevant features form audio signal, these features later help us to solve the problem in hand using machine learning or deep learning. Even though there are different types of feature but 2 most important types are **time domain and freq domain features**<br>\n",
    "\n",
    "**time domain** is basically the original waveform where we represent the wave in sine wave with amplitude vales in the y axis. some important features of time domain are **Amplitude envelope,Root-mean square energy,Zero crossing rate** <br>\n",
    "\n",
    "sometimes it is useful to look at waveform in **freq domain**. we take the original time domain waveform and apply fft(more on it later) on this to obtain freq domain representation.some important freq domain features are **Band energy ratio, Spectral centroid ,Spectral flux**.<br>\n",
    "\n",
    "There are features which represents both **time and freq domain** like Spectrogram,Mel-spectrogram,Constant-Q transform.<br>\n",
    "\n",
    "let's talk about the typical pipeline for time/freq domain feature extraction.\n",
    "\n",
    "**Time-domain feature pipeline**:<br>\n",
    "let's say we have a signal which was sampled at a freq of 44.1khz.now  we have a base signal of 44.1hz which means that we have 44.1k samples each sec. now in order to hear the 2 sample distinctly(human ear) we need a gap of **10ms**. if we take each sample for processing, each chunk(sample here) duration os 1/44100=0.0227 ms which is below the rate of **Perceivable audio chunk**. That is why we divide the signal into frames.\n",
    "\n",
    "**Frames**:<br>\n",
    "generally we use frames from 256-8192. let's say we want to find out duration of each frame. we need to know each frame size **K**. each sample duration is 1/sr. The formula is given below <br>\n",
    "\n",
    "$$df = (1/sr) * K $$\n",
    "\n",
    "One thing about frame is that we use frames of the power of 2(this helps in fft calculation)\n",
    "\n",
    "![time_domain_pipeline](./images/time_domain_pipeline.png)\n",
    "\n",
    "**Freq-domain feature pipeline**:<br>\n",
    "when we convert signal from time domain to freq domain we have a problem called **Spectral leakage.**\n",
    "Spectral leakage occurs when a non-integer number of periods of a signal is sent to the DFT. that's why the freq domain representation gets some high freq peaks.\n",
    "![spectral_leakage_fft](./images/spectral_leakage_fft.png)\n",
    "\n",
    "Inorder to solve the Spectral leakage problem we use something called **windowing function** for each frame.\n",
    "![windowing](./images/windowing.png)\n",
    "\n",
    "but windowing gives rise to one more problem. when we join multiple frames we lose information around the sides. \n",
    "![windowing_problem](./images/windowing_problem.png)\n",
    "\n",
    "so to sove this finally we use something called **overlapping frames**\n",
    "![overlapping](./images/overlapping.png)\n",
    "\n",
    "so the final Frequency-domain feature pipeline looks like below\n",
    "![freq_domain_pipeline](./images/freq_domain_pipeline.png)\n",
    "\n",
    "**NOTE**: To understand the spectral leakage and windowing please refer to this video as i feel the above explanation is not sufficient https://www.youtube.com/watch?v=tCWU9C-LdJQ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio",
   "language": "python",
   "name": "audio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
